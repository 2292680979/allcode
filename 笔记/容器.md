Arraylist

1. ArrayList与LinkedList的区别：1.底层实现，2.插入，删除，查找操作，3.线程安全

   4.内存空间占用

2. ArrayList：有序，允许null和重复，不是线程安全的（可以使用Collections包装）

3. Arraylist大小：没有指定大小时为10（先声明为size为0的数组，再在第一个add时初始化大小为10），小于0抛出异常，大于或等于0就设置为这个，输入容器时先把容器转换为数组，再判断大小，系统规定arraylist数组容量最大值为：MAX_ARRAY_SIZE = 2147483639

4. Arraylist扩容机制：1.判断是否是length为0的默认初始化的数组（即没有指定大小的数组），如果是初始化大小为10，2.判断容量是否够用，如果不够每次都扩容为原来（length）的1.5倍，3.判断1.5倍是否满足需求（size+1），不满足直接扩容为需求值   1.5倍是：oldCapacity + (oldCapacity >> 1) 3->3+1=4

5. Arraylist如何扩容的：生成一个新的length的数组，把原始数据复制到新数组

6. Remove操作：范围检查，remove操作后面的每一个元素集体向前移动一位覆盖remove位置上的元素，把size-1位置上的元素变为null  add操作：判断是否需要扩容，将元素加到最后一位  get操作：判断index是否符合要求，直接取index位置上的元素  set操作： 与get相同  contains操作：循环判断是否存在

7. Transient：因为实际的length与size不相等，所以序列化时就不用序列号哪些length-size后面的元素，所以先序列化非transient的东西，序列化elementData时，只序列化size大小的元素（加快了序列化速度，减少空间浪费）（被transient关键字修饰的变量不再能被序列化。）

8. 实现RandomAccess接口并没有方法，只是标识这个类有随机访问的功能（数组）

9. List，set，map之间的区别：list元素有序，可以重复。Set元素无序，不能重复。Map元素无序，key值不能重复。

10. Collection与Collections的区别：Collection接口是java集合的基本接口，他提供了集合对象进行操作的通用接口方法（如获取迭代器，add方法等），list/set都是它的子类（Map接口是map的顶层接口）。Collections是一个包装类，包含了很多静态方法，不能被实例化，就像一个工具类，提供排序方法，构造同步容器的方法等。

11. ArrayList与Vector的区别：后者是线程安全的，使用了Syn包裹，而ArrayList非线程安全，ArrayList性能更好（因为没有锁），Vector扩容每次增加1倍，而ArrayList每次增加0.5.

12. Arraylist没有缩容，数组的长度永远不会减少。Remove方法和clear方法只是会把相关位置的值置为null，以便垃圾回收

HashMap

​       https://www.zhihu.com/collection/518095252

1. 初始化：Node[] table默认长度为16，负载因子默认为0.75，threshold=table.length*factor，threshold为所能容纳键值对的个数，超过threshold要resize扩容  初始化传入值时，实际的length为大于或等于传入值的2的幂次方的数，如new HashMap(10)，则实际length为16，如果传入16，则length为16

2. Hashmap：无序，允许一条key为null，多条value为null，key值不能重复，访问速度块，非线程安全（Collections或者Concurrenthashmap）,**key值是不可变类型**（即hashcode()得出的值不能改变，因为如果改变，取值就就找不到这个值了）

3. HashSet：HashSet 是基于 HashMap 实现的，HashSet 底层使用 HashMap 来保存所有元素，因此 HashSet 的实现比较简单，相关 HashSet 的操作，基本上都是直接调用底层 HashMap 的相关方法来完成，HashSet 不允许重复的值。（HashSet的add操作就是map.put(key,PRESENT)，PRESENT是一个static Object类）  TreeSet：有序的set集合，内部有一个TreeMap，同样是调用TreeMap的方法来实现相关方法，如add（e）方法就是map.put(e, PRESENT)。 （**所以set都是基于map实现的**）

4. Hashtable线程安全，并发性太低(因为都是只用syn包裹)  linkedhashmap：保存了记录插入的顺序  **TreeMap**：可以根据键值排序，key必须实现comparable接口或者向treemap中传入comparator（treemap内部是完全由红黑树实现的，所以他的内部是排好序的，它的查找时间复杂度不是1，而是logn，插入时，判断key相同会覆盖，以此达到去重的效果）

5. Put操作：1判断table是否为null，是则**扩容**，key的hash值传播高位到低位运算（异或，右移）和取模运算（&运算）确定数组**位置** 2.判断此位置是否有元素，没有则直接插入，有则比较key值是否相等，相等则**更新value值** 3.不相等则判断此位置是否为红黑树，是则插入红黑树中，不是则插入链表中，插入链表后判断链表长度是否大于或等于8，是则**转化为红黑树**  4.判断size是否为threshold，是则**扩容**

 

6. 扩容机制（resize）：会伴随着一次重新hash分配，并且会遍历hash表中所有的元素，是非常耗时的。在编写程序中，要尽量避免resize。步骤：1.如果table为null，则**初始化table**，返回即可，2.判断length是否大于等于最大值，是则不再扩充直接返回，没有则扩充为**原来的2倍**（length和threshold都乘2）  3. 遍历oldtable，如果某个**节点没有next**，则直接取模运算得出新的index并赋值给newtable，4.节点为**红黑树**，则用红黑树的算法，5.如果节点为**链表**：判断e.hash&oldCap是否为0（e.hash&(oldCap-1)是取模运算，别搞混了），是0说明位置不变，不是0说明位置变为原位置+oldlength（为什么？实际上length*2就是length的二进制的1左移了一位，即length-1的二进制多出了一个1，这个1可能起作用（原位置+length），也可能不起作用（原位置），比如1010&111和1010&1111是不同的，而0010&111与0010&1111结果相同，所以我们只需判断第四位是否为1即可，即1010&1000为1000，说明1010位置会变，而0010&1000为0，说明0010位置不变），运用这种算法比重新计算index更高效（而重新计算index也是这两种情况，要么原位置，要么原位置+oldlength）

7. Get方法：1.传播高位到低位运算与取模运算计算数组位置 2.如果table为null或者此位置元素为null返回null  3.判断数组中此位置key值是否等于传入的key值，是则返回value  4.不是则查找红黑树或者链表，找到则返回，没有返回null

8. 为什么长度是2的幂次方？如果length为2的幂次方，那么，hash%length==hash&(length-1)，采用&运算比%运算更快  如10=1010 15=1111 ，10&15=10即10%16=10  2的幂次方的32位二进制只有1个1，其他都是0，如2：10，4：100，8：1000，16：10000，32：100000，所以二进制减1后都是1，如8-1=7=111，而1与任何数&操作都是那个数

9. 插入链表由头插法变为尾插法，避免了并发场景下链表成环的问题：两个线程put操作（原链表：1->2->3），一个resize了形成了一个新的table（而因为头插法，新的链表成了(1->3->2），一个没有导致一个线程中2.next指向3，而3.next指向2，成了一个环，在这个线程resize的时候就会死循环了。但是hashmap没有解决数据丢失的问题：即在插入时，两个线程都判断某个位置为null可以插入，第一个线程插入后，第二个线程也插入了，那么第一个线程插入的数据就丢失了

10. 负载因子：如果内存空间很多要提高效率，可以降低factor的值，如果内存紧张，而效率要求不高可以提高factor的值（因为，length相同，factor越大，存储的数据越多，碰撞的次数越多），为什么要选择0.75？是时间和空间的平衡，根据泊松分布，在负载因子默认为0.75的时候，单个hash槽内元素个数为8的概率小于百万分之一，所以将7作为一个分水岭，等于7的时候不转换，put操作时大于等于8的时候才进行转换，remove操作时小于等于6的时候红黑树就化为链表。

11. 为什么要扩容：减少hash冲突的概率

12. 为什么要转化为红黑树：红黑树(logn)比链表(n)查找更快

13. 为什么hash不用hashcode，而要多加一些运算呢？首先，我们知道index为（length-1）&hash，而length-1显然为一个前面都是0，后面都是1的32位值，这样的话，如果只用hashcode与length-1与运算，那么hashcode的前几位完全没有参与运算，如果hashcode是一个后几位都是0的值，那么很容易发生碰撞。而先给hashcode高位运算，可以将原先不参与运算的高位也给包含进来了（当只有高位有1时，高位运算把高位推向了低位，当只有低位有1时，高位运算对他没有改变）

14. Hashmap1.7与1.8的区别：1.8增加了红黑树，而1.7只有链表。插入链表时，由头插法变为了尾插法。

15. 遍历map：entrySet()获得所有键值对，keySet()获得所有key，values()获得所有value。

ConcurrentHashMap

1. Hashtable效率低的原因：他对数据操作的时候都会用syn对方法上锁，不论是set还是get，所以效率比较低下，Collections.synchronizedMap同样

2. Hashtable的不允许key或者value为null，直接会抛出空指针异常，为什么呢？如果你使用null值，就会使得其无法判断对应的key是不存在还是为空，因为你无法再调用一次contains(key）来对key是否存在进行判断（原来key=‘1‘是不存在的。这时，两个线程，一个线程调用get(‘1’)发现返回是null，于是调用contains(’1‘)判断是不存在还是value为null，而另一个线程此时插入了key=’1’的键值对，那么显然第一个线程返回的信息是错误的），ConcurrentHashMap同理。

3. 安全失败机制：concurrent包下面的容器都是安全失败，可以在多线程下并发遍历，并发修改。原理：在遍历（注意只有在使用迭代器遍历时才会，使用get不会，因为volatile可见性，直接使用get不会有安全问题）时不是直接在集合内容上访问的，而是先复制原有集合内容，在拷贝的集合上进行遍历，所以遍历时集合的改变检测不到

4. 快速失败：在用迭代器遍历容器时，如果容器内容发生了改变，会抛出异常，原理：hashmap中有一个modCount变量，集合每次改变，modCount++，每次迭代器迭代时，都会检测modCound的值，与期望值不相同，则抛出异常

5. Hashtable与hashmap的不同：1.初始容量：11，16 2.扩容机制：当前容量翻倍+1，容量翻倍  3.迭代器不同，不是快速失败，是快速失败。

6. Concurrenthashmap1.7（一个ConcurrentHashMap里包含一个Segment数组，每个Segment里包含一个HashEntry数组，我们称之为table，每个HashEntry是一个链表结构的元素。 Segment[] segments, Segment类继承reentrantlock，Segment类属性：HashEntry[] table，hashentry类属性包含key，value，next）： put方法：1.根据key的hash值定位到某个segment  2.自旋**获取当前segment的锁**  3. 计算在HashEntry数组中的位置，判断此此位置是否为null，不为null则遍历则位置上的链表，更新或者插入链表  4. 为空则需要新建一个 HashEntry 并加入到 hashentry数组中 中，同时会先判断是否需要扩容 5.释放锁   所以他是对每个segment加锁，则理论上数组有多长，此concurrenthashmap的并发度就有多高（默认16）   get方法：不加锁，非常高效

​                            ![](图片\image-20200723111808620-1596264685305.png)   

​								![image-20200723111843578](图片\image-20200723111843578-1596264687510.png)

 

 

7. 1.8（Node[] table, node继承hashMap的Entry）：内部结构与hashmap相同，初始数组大小也为16，链表大于8时转化为红黑树 ，扩容也是2倍  1.7采用分段锁，1.8之后用cas思想代替分段锁，核心就是尽量降低同步锁的粒度（主要区别：红黑树，分段锁（syn+CAS，lock），尾插法）

8. Put方法（初始化，定位，自旋，扩容，写入，转化，大小，扩容）：1.判断table是否初始化  2.定位到key在table中的位置，判断此位置是否为null，为null则用CAS自旋写入数据  3.如果不为null判断是否正在被别的线程扩容（即table定位到的节点的hash值是否为-1），是则帮助扩容  4.不为null也没在扩容则用syn加锁（为node加锁，相当于segment）写入数据（链表(hash>=0)或红黑树（hash<0，treebin的值为-2，treebin代表的是红黑树的根节点的hash值）），退出syn  5.判断链表node数是否达到8，达到则转化为红黑树  6.容器大小加一，判断是否需要扩容，如果需要就扩容，如果正在扩容，就帮助扩容  同样，get方法也没有加锁

9. Put方法的第一步就是给key值再hash，即执行spread（）方法，算出key的hash值，以后使用的都是这个hash值，而不是hashcode()计算出的值（hashmap中的hash值也是再hash的值）

10. 初始化table：table为null时，自旋判断**sizeCtl是否小于0**（小于0说明此table正在**被其他线程初始化**或者扩容），是则**让出cpu**（即线程让步），不是则用cas操作给sizeCtl赋值为-1，并再次判断table是否为null，是则**初始化table**，最后重新给sizeCtl赋值（赋正值，sizeCtl表示：-1，表示正在初始化，-n，表示正在由n-1个线程执行扩容操作，正值为此容器的容量即length*0.75或者length-length>>2）

11. Size()方法：baseCount加上counterCells数组的所有非null值得出

12. helpTransfer()帮助扩容：1.如果table不是null，put方法传进来的node节点是转移类型，node的新table不是null，则尝试帮助扩容 2.帮助扩容：根据length得到标识符rs，循环检测：如果oldtable没变，newtable没变（相比1得到的没变），且sizeCtl小于0  3.再判断，如果扩容没有结束，没有达到最大线程数，则帮助扩容（执行transfer方法），否则退出循环退出方法

    https://segmentfault.com/a/1190000014982330 map各个方法分析

13. addCount()数组容量加1且判断是否扩容：1. 判断计数盒子属性是否是空，如果是空，就尝试修改 **baseCount 变量，对该变量进行加 X**（默认为1），不为null直接第3步 2. 如果计数盒子不是空，或者修改 baseCount 变量失败了，则放弃对 baseCount 进行操作。 3.如果计数盒子不是null，且长度不为0，且计数盒子上**随机取的一个元素不为null**，那么，就执行cas操作把这个元素的**value值加上x**，上面任意一条不满足（即countercells为没有被初始化，或者没有插入一条数据，或者随机选取的位置上没有插入过数据，或者执行cas操作失败），就执行fulladdcound方法  4.fulladdcound：简而言之就是初始化countercells，或者初始化某个位置上的cell，或者更新此cell的value值，或者扩容countercells  5.判断是否需要扩容，如果正在扩容，则帮助扩容，如果需要扩容，就开始扩容   （**简而言之**：通过修改 baseCount，或者通过使用 CounterCell。当 CounterCell 数组被初始化了，就**优先使用他**，不再使用 baseCount。CounterCell是一个类，里面有一个volatile long类型的value）

14. 